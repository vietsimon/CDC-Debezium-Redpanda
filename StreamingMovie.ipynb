{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe764d65-82c3-4946-8e71-b37fbec6f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc,from_json,col\n",
    "import pyspark.sql.functions as psf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b816a8-b4eb-400f-93ac-d21dfe421a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.17, OpenJDK 64-Bit Server VM, 1.8.0_92\n",
      "Branch HEAD\n",
      "Compiled by user centos on 2023-06-19T23:01:01Z\n",
      "Revision 6b1ff22dde1ead51cbf370be6e48a802daae58b6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7211b0a-6eff-4b86-8072-e38a0e41c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "spark_version = '3.4.1'\n",
    "scala_version = '2.12'\n",
    "# TODO: Ensure match above values match the correct versions\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.5.1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163cbba8-121a-4458-96dd-e210a0ca22c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kafka2/miniconda3/envs/myenv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kafka2/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kafka2/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-795e9819-2c84-40ba-aea7-1589ab3fe158;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.5.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      ":: resolution report :: resolve 429ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 by [org.apache.kafka#kafka-clients;3.5.1] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.6] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   2   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-795e9819-2c84-40ba-aea7-1589ab3fe158\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/9ms)\n",
      "24/02/26 15:39:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession\\\n",
    ".builder\\\n",
    ".master(\"local[1]\")\\\n",
    ".appName(\"Spark API\")\\\n",
    ".config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    ".config(\"spark.jars\", \"/home/kafka2/documents/orderManagement/postgresql-42.6.0.jar\") \\\n",
    ".getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df31abd5-a596-4def-991b-722a4ab8341c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://node-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark API</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd1c641b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb94f87-df46-49a3-aecb-ff96dfa3eda3",
   "metadata": {},
   "source": [
    "## Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8671b-f112-4618-b838-ee9e932098fd",
   "metadata": {},
   "source": [
    "- **movie_info** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99aab1eb-8f0c-4998-9f6b-baa8fb5f4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = spark \\\n",
    ".readStream \\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"192.168.1.14:9092\") \\\n",
    ".option(\"subscribe\", \"fullfill27.test1.movie_info\") \\\n",
    ".option(\"startingOffsets\", \"earliest\") \\\n",
    ".load()\\\n",
    ".selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa925c0a-f87e-4e34-a7c5-b67d3f2dc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=movie_df.select(psf.get_json_object(movie_df['value'],\"$.payload\").alias('payload'))\n",
    "temp1=temp1.select(psf.get_json_object(temp1['payload'],\"$.before\").alias('before'),\\\n",
    "                   psf.get_json_object(temp1['payload'],\"$.after\").alias('after'),\\\n",
    "                   psf.get_json_object(temp1['payload'],\"$.op\").alias('operation'),\\\n",
    "                   psf.get_json_object(temp1['payload'],\"$.ts_ms\").alias('timestamp debezium'),\\\n",
    "                  psf.get_json_object(temp1['payload'],\"$.source\").alias('source'))\n",
    "\n",
    "temp_movie=temp1.select(\"before\",\"after\",\"operation\",psf.get_json_object(temp1['source'],\"$.ts_ms\").alias('timestamp DB'),\"timestamp debezium\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89d25f-2a5f-447a-b6e9-7b8f11bdd565",
   "metadata": {},
   "source": [
    "- **user_ratings** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b56231-a61e-4ab4-a1df-2d21b2acf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df = spark \\\n",
    ".readStream \\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"192.168.1.14:9092\") \\\n",
    ".option(\"subscribe\", \"fullfill27.test1.user_ratings\") \\\n",
    ".option(\"startingOffsets\", \"earliest\") \\\n",
    ".load()\\\n",
    ".selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaaf9fd3-ddc2-4a5e-9927-645d2c639263",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=user_ratings_df.select(psf.get_json_object(user_ratings_df['value'],\"$.payload\").alias('payload'))\n",
    "temp1=temp1.select(psf.get_json_object(temp1['payload'],\"$.before\").alias('before'),\\\n",
    "                   psf.get_json_object(temp1['payload'],\"$.after\").alias('after'),\\\n",
    "                   psf.get_json_object(temp1['payload'],\"$.op\").alias('operation'),\\\n",
    "                   psf.get_json_object(temp1['payload'],\"$.ts_ms\").alias('timestamp debezium'),\\\n",
    "                  psf.get_json_object(temp1['payload'],\"$.source\").alias('source'))\n",
    "\n",
    "temp_user_ratings=temp1.select(\"before\",\"after\",\"operation\",psf.get_json_object(temp1['source'],\"$.ts_ms\").alias('timestamp DB'),\"timestamp debezium\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116d0ff-1d1a-4a64-bd5f-37624827177c",
   "metadata": {},
   "source": [
    "## Spark Streaming Sink to PostgresSql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7eeddd-e760-4e0c-9c8b-785be8f015ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to write data to postgres\n",
    "def write_to_postgresql_movie(df,epoch_id):\n",
    "    df.write \\\n",
    "    .format('jdbc') \\\n",
    "    .options(url='jdbc:postgresql://192.168.1.14/temp',\n",
    "            driver='org.postgresql.Driver',\n",
    "            dbtable='movie_info',\n",
    "            user='postgres',\n",
    "            password='Nintendo1',\n",
    "            ) \\\n",
    "    .mode('append') \\\n",
    "    .save()\n",
    "    \n",
    "def write_to_postgresql_user_ratings(df,epoch_id):\n",
    "    df.write \\\n",
    "    .format('jdbc') \\\n",
    "    .options(url='jdbc:postgresql://192.168.1.14/temp',\n",
    "            driver='org.postgresql.Driver',\n",
    "            dbtable='user_ratings',\n",
    "            user='postgres',\n",
    "            password='Nintendo1',\n",
    "            ) \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b97d2-cc5b-4940-9a28-41610e96f106",
   "metadata": {},
   "source": [
    "-  **foreachBatch** in **PySpark** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56cab1f-49f0-42ff-9471-e748c0e79972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 15:39:24 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9701da6b-a17b-4551-8637-8f00431a3e75. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/02/26 15:39:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/02/26 15:39:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-caa2aa9b-bdd9-4b59-a11b-de723bdebbc5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/02/26 15:39:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/02/26 15:39:44 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 18926 milliseconds\n",
      "24/02/26 15:40:13 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 47638 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query1=temp_movie.writeStream \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .outputMode('Append') \\\n",
    "    .foreachBatch(write_to_postgresql_movie) \\\n",
    "    .start()\n",
    "\n",
    "query2=temp_user_ratings.writeStream \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .outputMode('Append') \\\n",
    "    .foreachBatch(write_to_postgresql_user_ratings) \\\n",
    "    .start()\n",
    "spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29f4b1-d66a-4367-856f-113693468ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
